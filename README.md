# Lasso-Regression-Model
Lasso regression projeckt


---

## ๐ **ุดุฑุญ Lasso Regression (ุงูุงูุญุฏุงุฑ ูุน ุงูุนููุจุฉ L1)**

### ๐ ูุง ูู Lasso Regressionุ

Lasso Regression ูู ูููุฐุฌ ุงูุญุฏุงุฑ ุฎุทู ูุทูุฑ ูุถูู **ุนููุจุฉ (Regularization)** ูู ููุน **L1** ุฅูู ุฏุงูุฉ ุงูุฎุณุงุฑุฉ (Loss Function).
โ ุงููุฏู ุงูุฃุณุงุณู ููู ูู:

* ุชูููู ุงูุชุนููุฏ (Complexity) ูู ุงููููุฐุฌ.
* **ุงุฎุชูุงุฑ ุงููููุฒุงุช ุงููููุฉ ููุท** (Feature Selection) ุนู ุทุฑูู ุฌุนู ุจุนุถ ุงูุฃูุฒุงู = ุตูุฑ.
* ูุนุงูุฌุฉ ูุดููุฉ **Overfitting**.

---

### ๐งฎ **ุงููุนุงุฏูุฉ ุงูุฃุณุงุณูุฉ**

#### โช๏ธ 1. ุฏุงูุฉ ุงูุงูุญุฏุงุฑ ุงูุนุงุฏูุฉ:

$$
y = b + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

#### ๐ด 2. ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุนุงุฏูุฉ (MSE):

$$
\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

#### ๐ข 3. ุฏุงูุฉ ุงูุฎุณุงุฑุฉ ูุน Lasso (L1 Regularization):

$$
\text{Loss}_{\text{Lasso}} = \text{MSE} + \lambda \sum_{j=1}^{n}|w_j|
$$

* $\lambda$: ูุนุงูู ุงูุชุญูู ูู ุงูุนููุจุฉ (Regularization Strength).
* $|w_j|$: ุงููููุฉ ุงููุทููุฉ ููุฃูุฒุงู.

---

### โ๏ธ **ููู ุชุนูู L1 Regularizationุ**

* **ุนูุฏ ุชุญุฏูุซ ุงูุฃูุฒุงู** ุจุงุณุชุฎุฏุงู **Gradient Descent**ุ ูุชู ุชุนุฏูู ูุนุงุฏูุฉ ุงูุชุญุฏูุซ:

$$
w_j = w_j - \alpha \left(\frac{\partial \text{Loss}}{\partial w_j}\right)
$$

โ ูุน L1:

$$
\frac{\partial \text{Loss}}{\partial w_j} = \frac{\partial \text{MSE}}{\partial w_j} + \lambda \cdot \text{sign}(w_j)
$$

* $\text{sign}(w_j)$: ุฅุดุงุฑุฉ ุงููุฒู (ููุฌุจุฉ ุฃู ุณุงูุจุฉ).

---

### ๐ฏ **ูููุฒุงุช Lasso Regression**

โ ูุฎุชุงุฑ ุงููููุฒุงุช ุงููููุฉ ููุท ุชููุงุฆููุง (ุจุนุถ ุงูุฃูุฒุงู ุชุตุจุญ ุตูุฑูุง).
โ ูููู ุงูุชุนููุฏ ูู ุงูููุงุฐุฌ ุฐุงุช ุงููููุฒุงุช ุงููุซูุฑุฉ.
โ ูููุน Overfitting ุนูู ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ.

---

### ๐จ **ุงููุฑู ุจูู Lasso ู Ridge**

|               | **Lasso (L1)**             | **Ridge (L2)**             |   |              |
| ------------- | -------------------------- | -------------------------- | - | ------------ |
| **ุงูุนููุจุฉ**   | (\sum                      | w\_j                       | ) | $\sum w_j^2$ |
| **ุงูุฃูุฒุงู**   | ูุฌุจุฑ ุจุนุถ ุงูุฃูุฒุงู ุนูู ุงูุตูุฑ | ูููู ุงูุฃูุฒุงู ููู ูุง ูุตูุฑูุง |   |              |
| **ุงูุงุณุชุฎุฏุงู** | ููุงุณุจ ูุงุฎุชูุงุฑ ุงููููุฒุงุช     | ููุงุณุจ ุนูุฏ ูุฌูุฏ ูู ุงููููุฒุงุช |   |              |

---

### ๐๏ธ **ูุซุงู ุจุฑูุฌู (Lasso Regression ูู ุงูุตูุฑ)**

```python
import numpy as np

# ุจูุงูุงุช
x = [1, 2, 3]
y = [2, 4, 6]

# ูุนุงููุงุช ุงูุจุฏุงูุฉ
w = 0.01
b = 0
alpha = 0.1  # ูุนุฏู ุงูุชุนูู
lmbda = 0.1  # ูุนุงูู ุงูุนููุจุฉ

# ุงูุชุฏุฑูุจ
for epoch in range(100):
    total_error_w = 0
    total_error_b = 0
    for i in range(len(x)):
        y_pred = b + w * x[i]
        error = y_pred - y[i]
        total_error_w += error * x[i]
        total_error_b += error

    # ุญุณุงุจ ุงูุชุฏุฑุฌุงุช
    grad_w = (2 / len(x)) * total_error_w + lmbda * np.sign(w)
    grad_b = (2 / len(x)) * total_error_b

    # ุชุญุฏูุซ ุงููุนุงููุงุช
    w -= alpha * grad_w
    b -= alpha * grad_b

    if epoch % 10 == 0:
        print(f"Epoch {epoch}: w={w:.4f}, b={b:.4f}")

print(f"\nโ ุงููุชูุฌุฉ ุงูููุงุฆูุฉ: w={w:.4f}, b={b:.4f}")
```

---

### โ **ุงูุฎูุงุตุฉ**

* **Lasso Regression** ูุซุงูู ุฅุฐุง ููุช ุชุฑูุฏ ูููุฐุฌูุง ุฃุจุณุท ูููู ุจุงุฎุชูุงุฑ ุฃูู ุงููููุฒุงุช ููุท.
* ุนู ุทุฑูู ุงูุชุญูู ูู $\lambda$ุ ููููู ุชุญุฏูุฏ ูุฏู ุงูุนููุจุฉ ุงูููุฑูุถุฉ ุนูู ุงูุฃูุฒุงู.

---

## ๐ **ุฅู ุฃุนุฌุจู ุงูุดุฑุญ ูุง ุชูุณู ูุถุน Star ูููุณุชูุฏุน ุนูู GitHub!**

---

### ๐ ูู ุชุฑูุฏูู:


